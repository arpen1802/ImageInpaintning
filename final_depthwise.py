# -*- coding: utf-8 -*-
"""Final DepthWise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mMSfzBnsPBlcfFmK7GIdndvT1xklgyC1
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.nn.utils import spectral_norm
from torchvision import datasets, transforms
from torchvision.utils import save_image, make_grid
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from skimage.measure import compare_ssim as ssim
import os
from torch.utils.tensorboard import SummaryWriter
# %load_ext tensorboard
torch.manual_seed(1)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(0)

class Encoder(nn.Module):
    """
    Encoder module used in PEPSI paper
    """
    def __init__(self, cnum=32):
        super(Encoder, self).__init__()
        self.cnum = cnum
        # necessary paddings
        self.pad1 = nn.ReflectionPad2d((1, 1, 1, 1))
        self.pad2 = nn.ReflectionPad2d((2, 2, 2, 2))
        self.pad4 = nn.ReflectionPad2d((4, 4, 4, 4))
        self.pad8 = nn.ReflectionPad2d((8, 8, 8, 8))
        self.pad16 = nn.ReflectionPad2d((16, 16, 16, 16))
        # Model convolutions
        self.conv1_depth = nn.Conv2d(6, 6, [5, 5], stride=[1, 1], padding=0, groups=6)
        self.conv1_sep = nn.Conv2d(6, cnum, 1, stride=[1, 1], padding=0)
        self.conv2_depth = nn.Conv2d(cnum, cnum, [3, 3], stride=[2, 2], padding=0, groups=cnum)
        self.conv2_sep = nn.Conv2d(cnum, cnum*2, 1, stride=1, padding=0)
        self.conv3_depth = nn.Conv2d(cnum*2, cnum*2, [3, 3], stride=[1, 1], padding=0, groups=cnum*2)
        self.conv3_sep = nn.Conv2d(cnum*2, cnum*2, 1, stride=1, padding=0)
        self.conv4_depth = nn.Conv2d(cnum*2, cnum*2, [3, 3], stride=[2, 2], padding=0, groups=cnum*2)
        self.conv4_sep = nn.Conv2d(cnum*2,cnum*4, 1, stride=1, padding=0)
        self.conv5_depth = nn.Conv2d(cnum*4, cnum*4, [3, 3], stride=[1, 1], padding=0, groups=cnum*4)
        self.conv5_sep = nn.Conv2d(cnum*4,cnum*4, 1, stride=1, padding=0)
        self.conv6_depth = nn.Conv2d(cnum*4, cnum*4, [3, 3], stride=[2, 2], padding=0, groups=cnum*4)
        self.conv6_sep = nn.Conv2d(cnum*4,cnum*8, 1, stride=1, padding=0)
        self.dil_conv1_depth = nn.Conv2d(cnum*8, cnum*8, [3, 3], stride=[1, 1], padding=0, groups=cnum*8,dilation=2)
        self.dil_conv1_sep = nn.Conv2d(cnum*8, cnum*8, 1, stride=[1, 1], padding=0)
        self.dil_conv2_depth = nn.Conv2d(cnum*8, cnum*8, [3, 3], stride=[1, 1], padding=0, groups=cnum*8,dilation=4)
        self.dil_conv2_sep = nn.Conv2d(cnum*8, cnum*8, 1, stride=[1, 1], padding=0)
        self.dil_conv3_depth = nn.Conv2d(cnum*8, cnum*8, [3, 3], stride=[1, 1], padding=0, groups=cnum*8,dilation=8)
        self.dil_conv3_sep = nn.Conv2d(cnum*8, cnum*8, 1, stride=[1, 1], padding=0)
        self.dil_conv4_depth = nn.Conv2d(cnum*8, cnum*8, [3, 3], stride=[1, 1], padding=0, groups=cnum*8,dilation=16)
        self.dil_conv4_sep = nn.Conv2d(cnum*8, cnum*8, 1, stride=[1, 1], padding=0)
        self.act = nn.ELU()
        
    def forward(self, x):
        """
        Forward of Encoder
        """
        x = self.pad2(x)
        x = self.conv1_depth(x)
        x = self.act(self.conv1_sep(x))
        x = self.pad1(x)
        x = self.conv2_depth(x)
        x = self.act(self.conv2_sep(x))
        x = self.pad1(x)
        x = self.conv3_depth(x)
        x = self.act(self.conv3_sep(x))
        x = self.pad1(x)
        x = self.conv4_depth(x)
        x = self.act(self.conv4_sep(x))
        x = self.pad1(x)
        x = self.conv5_depth(x)
        x = self.act(self.conv5_sep(x))
        x = self.pad1(x)
        x = self.conv6_depth(x)
        x = self.act(self.conv6_sep(x))
        x = self.pad2(x)
        x = self.dil_conv1_depth(x)
        x = self.act(self.dil_conv1_sep(x))
        x = self.pad4(x)
        x = self.dil_conv2_depth(x)
        x = self.act(self.dil_conv2_sep(x))
        x = self.pad8(x)
        x = self.dil_conv3_depth(x)
        x = self.act(self.dil_conv3_sep(x))
        x = self.pad16(x)
        x = self.dil_conv4_depth(x)
        x = self.act(self.dil_conv4_sep(x))
        return x

class ContextualBlock(nn.Module):
    """Contextual Attention Module (CAM)
    """
    def __init__(self, softmax_scale=50):

        super(ContextualBlock, self).__init__()
        # Default value for softmax_scale=50 is taken from official implementation
        self.softmax_scale = softmax_scale
        self.patches1 = torch.nn.Unfold(kernel_size=3, stride=1, dilation=1, padding=0)
        self.patches2 = torch.nn.Unfold(kernel_size=3, stride=1, dilation=1, padding=1)
        self.conv = torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1)
        self.act = torch.nn.ELU()
        
    def forward(self, back, foregr, mask):
        """
        background and foreground are the same output of the encoder 
        Params:
            back - background i.e. not masked part
            foregr - foreground i.e. masked part
            mask - mask used for masking the image
        """
        b, dims, h, w = back.size()
        #  downsample mask as output of encoder is smaller than mask
        temp = F.interpolate(mask, size=(h,w), mode='nearest')
        temp = temp[:,0,:,:].unsqueeze(dim=1)
        mask_r = temp.repeat([1, dims, 1, 1])
        # mask all featuremaps of background
        bg = back * (1-mask_r)
        k_size = 3 
        # extract patches from background
        # there will be 900 pathches of 256x3x3 size
        patch1 = self.patches1(bg)
        patch1 = patch1.reshape(b, 256, 3, 3, 900)
        patch1 = patch1.permute(0, 4, 1, 2, 3)
        # extract patches from foreground
        # there will be 1024 patches of size 256x3x3
        patch2 = self.patches2(foregr)
        patch2 = patch2.reshape(b, 32, 32, 2304)
        ACL = []
        for ib in range(b):
            # for each image in batch
            # take patches of background 
            # and do convolution on foreground
            # using patches as convolutional filters
            k1 = patch1[ib, :, :, :, :]
            ww = foregr[ib, :, :, :]
            ft = torch.unsqueeze(ww, 0)
            CS = F.conv2d(ft, k1, stride=1, padding=1) 
            # Calcualate Euclidean distance between patches
            k1d = torch.sum(torch.pow(k1, 2), axis=[1,2,3], keepdims=False)
            k1d = k1d.unsqueeze(0).unsqueeze(0)
            wwd = torch.sum(torch.pow(patch2[ib, :, :, :], 2), axis=2, keepdims=True)
            tt = k1d + wwd
            tt = tt.unsqueeze(0).permute(0, 3, 1, 2)
            DS1 = tt - 2 * CS
            DS2 = (DS1 - torch.mean(DS1, 1, True)) / torch.std(DS1, 1, True)
            DS2 = -1 * torch.tanh(DS2)
            # Softmax the calculated similarity score
            # -3 part is from PEPSI implementation
            CA =  F.softmax(DS2*self.softmax_scale - 3, dim=1) 
            # Reconstruct the image using background pathces
            # by weighting them by the CA softmaxed scores
            ACLt = F.conv_transpose2d(CA, k1, stride=1, padding=1)
            ACLt = ACLt / (k_size ** 2)
            ACL.append(ACLt) 
        # back to minibatch
        ACL = torch.cat(ACL, dim=0) 
        # as background was masked, put the reconstructed part
        # in background's masked place
        ACL = bg + ACL * mask_r
        # concat them depthwise and convolve by 1x1 kernel
        con1 = torch.cat([back, ACL], 1)
        ACL2 = self.act(self.conv(con1))
        return ACL2

class Decoder(nn.Module):
    """
    Decoder module used in PEPSI paper
    """
    def __init__(self, cnum=256, size1=256, size2=256):
        super(Decoder, self).__init__()
        self.size1 = size1
        self.size2 = size2     
        # Necessary padding
        self.pad1 = nn.ReflectionPad2d((1, 1, 1, 1))
        # Model convolutions
        self.conv1_depth = nn.Conv2d(cnum, cnum, [3, 3], stride=[1, 1], padding=0, groups=cnum)
        self.conv1_sep = nn.Conv2d(cnum, cnum//2, 1, stride=[1, 1], padding=0)
        self.conv2_depth = nn.Conv2d(cnum//2, cnum//2, [3, 3], stride=[1, 1], padding=0, groups=cnum//2)
        self.conv2_sep = nn.Conv2d(cnum//2, cnum//2, 1, stride=[1, 1], padding=0)
        self.conv3_depth = nn.Conv2d(cnum//2, cnum//2, [3, 3], stride=[1, 1], padding=0, groups=cnum//2)
        self.conv3_sep = nn.Conv2d(cnum//2, cnum//4, 1, stride=[1, 1], padding=0)
        self.conv4_depth = nn.Conv2d(cnum//4, cnum//4, [3, 3], stride=[1, 1], padding=0, groups=cnum//4)
        self.conv4_sep = nn.Conv2d(cnum//4, cnum//4, 1, stride=[1, 1], padding=0)
        self.conv5_depth = nn.Conv2d(cnum//4, cnum//4, [3, 3], stride=[1, 1], padding=0, groups=cnum//4)
        self.conv5_sep = nn.Conv2d(cnum//4, cnum//8, 1, stride=[1, 1], padding=0)
        self.conv6_depth = nn.Conv2d(cnum//8, cnum//8, [3, 3], stride=[1, 1], padding=0, groups=cnum//8)
        self.conv6_sep = nn.Conv2d(cnum//8, cnum//8, 1, stride=[1, 1], padding=0)
        self.conv7_depth = nn.Conv2d(cnum//8, cnum//8, [3, 3], stride=[1, 1], padding=0, groups=cnum//8)
        self.conv7_sep = nn.Conv2d(cnum//8, cnum//16, 1, stride=[1, 1], padding=0)
        self.conv8_depth = nn.Conv2d(cnum//16, cnum//16, [3, 3], stride=[1, 1], padding=0, groups=cnum//16)
        self.conv8_sep = nn.Conv2d(cnum//16, cnum//16, 1, stride=[1, 1], padding=0)
        self.conv9_depth = nn.Conv2d(cnum//16, cnum//16, [3, 3], stride=[1, 1], padding=1, groups=cnum//16)
        self.conv9_sep = nn.Conv2d(cnum//16, 3, 1, stride=[1, 1], padding=0)
        # Model activation
        self.act = nn.ELU()

    def forward(self, x):
        """
        Forward of Decoder
        """
        x = self.pad1(x)
        x = self.conv1_depth(x)
        x = self.act(self.conv1_sep(x))
        x = self.pad1(x)
        x = self.conv2_depth(x)
        x = self.act(self.conv2_sep(x))
        x = F.interpolate(x, (self.size1//4, self.size2//4),mode='nearest')
        x = self.pad1(x)
        x = self.conv3_depth(x)
        x = self.act(self.conv3_sep(x))
        x = self.pad1(x)
        x = self.conv4_depth(x)
        x = self.act(self.conv4_sep(x))
        x = F.interpolate(x, (self.size1//2, self.size2//2),mode='nearest')
        x = self.pad1(x)
        x = self.conv5_depth(x)
        x = self.act(self.conv5_sep(x))
        x = self.pad1(x)
        x = self.conv6_depth(x)
        x = self.act(self.conv6_sep(x))
        x = F.interpolate(x, (self.size1, self.size2),mode='nearest')
        x = self.pad1(x)
        x = self.conv7_depth(x)
        x = self.act(self.conv7_sep(x))
        x = self.pad1(x)
        x = self.conv8_depth(x)
        x = self.act(self.conv8_sep(x))
        x = F.interpolate(x, (self.size1, self.size2),mode='nearest')
        x = self.conv9_depth(x)
        x = self.conv9_sep(x)
        x = torch.clamp(x, -1.0, 1.0)
        return x

class RedSN(nn.Module):
    """Last layer of Discriminator RED
    There should be 16 fully connected layers applied to each pixel depthwise
    It is equivalent to have one tensor of the same size as input, 
    do element-wise mulitpilication and sum depthwise.
    """
    def __init__(self, size):
        super(RedSN, self).__init__()
        self.weight = torch.nn.Parameter(nn.init.xavier_uniform_(torch.empty(size)))
        self.bias = torch.nn.Parameter(nn.init.constant_(torch.empty(1, 1, size[2], size[3]), 0))
    def forward(self, x):
        return torch.sum(x*self.weight, dim=1, keepdim=True) + self.bias

class Discriminator_red(nn.Module):
    """
    Region Ensemble Discriminator (RED) module used in PEPSI paper
    """
    def __init__(self, cnum = 64):
        super(Discriminator_red, self).__init__()

        self.conv1_depth =  spectral_norm(nn.Conv2d(3, 3, [5, 5], stride=[2, 2], padding=2, groups=3))
        self.conv1_sep =  spectral_norm(nn.Conv2d(3, cnum, 1, stride=1, padding=0))
        
        self.conv2_depth =  spectral_norm(nn.Conv2d(cnum, cnum, [5, 5], stride=[2, 2], padding=2, groups=cnum))
        self.conv2_sep =  spectral_norm(nn.Conv2d(cnum, cnum*2, 1, stride=1, padding=0))

        self.conv3_depth =  spectral_norm(nn.Conv2d(cnum*2, cnum*2, [5, 5], stride=[2, 2], padding=2, groups=cnum*2))
        self.conv3_sep =  spectral_norm(nn.Conv2d(cnum*2, cnum*4, 1, stride=1, padding=0))

        self.conv4_depth =  spectral_norm(nn.Conv2d(cnum*4, cnum*4, [5, 5], stride=[2, 2], padding=2, groups=cnum*4))
        self.conv4_sep =  spectral_norm(nn.Conv2d(cnum*4, cnum*4, 1, stride=1, padding=0))

        self.conv5_depth =  spectral_norm(nn.Conv2d(cnum*4, cnum*4, [5, 5], stride=[2, 2], padding=2, groups=cnum*4))
        self.conv5_sep =  spectral_norm(nn.Conv2d(cnum*4, cnum*4, 1, stride=1, padding=0))

        self.conv6_depth =  spectral_norm(nn.Conv2d(cnum*4, cnum*4, [5, 5], stride=[2, 2], padding=2, groups=cnum*4))
        self.conv6_sep =  spectral_norm(nn.Conv2d(cnum*4, cnum*8, 1, stride=1, padding=0))

        self.fc = spectral_norm(RedSN((1,cnum*8, 4, 4)))
        self.act = nn.LeakyReLU()
        
    def forward(self, x):
        x = self.conv1_depth(x)
        x = self.act(self.conv1_sep(x))
        x = self.conv2_depth(x)
        x = self.act(self.conv2_sep(x))
        x = self.conv3_depth(x)
        x = self.act(self.conv3_sep(x))
        x = self.conv4_depth(x)
        x = self.act(self.conv4_sep(x))
        x = self.conv5_depth(x)
        x = self.act(self.conv5_sep(x))
        x = self.conv6_depth(x)
        x = self.act(self.conv6_sep(x))
        x = self.fc(x)
        return x

class Inpaint(nn.Module):
    def __init__(self):
        super(Inpaint, self).__init__()
        self.encoder = Encoder()
        self.context_block = ContextualBlock()
        self.decoder = Decoder()
            
    def forward(self, x_mask, Y, mask, is_train=True):
        """
        Forward of Inpaint module. 
        Params:
            x_mask - masked image with mask concatenates depthwise
            Y - ground truth
            mask - mask
            is_train - whether to use coarse path or not. 
                       Is true for training, False for test
        """

        vec_en = self.encoder(x_mask)
        vec_con = self.context_block(vec_en, vec_en, mask)
        image_gen = self.decoder(vec_con)
        image_result = image_gen * mask  + Y * (1-mask)  
        if is_train:
            image_coarse = self.decoder(vec_en)
            return image_coarse, image_gen, image_result
        return image_result

def make_sq_mask(size, m_size):
    """
    Make a square mask
    Params:
        size - size of whole mask map
        m_size - size of the mask in the image
    """
    start_x = np.random.randint(0, size - m_size-1)
    start_y = np.random.randint(0, size - m_size-1)
    temp = np.zeros([size, size, 3])
    temp[start_x:start_x + m_size, start_y:start_y + m_size, 0:3] = 255
    return temp.astype(np.uint8)

# path where a folder with name dlr_x_glr_x will be created
# and all necessary info will be kekp
save_path = '/content/drive/My Drive/Colab/models_final' 
# path to train data
train_path = '/content/drive/My Drive/Colab/train_sixteen'
# path to test data
valid_path = '/content/drive/My Drive/Colab/train_sixteen'
# hyperparaters
d_lr = 0.0006 # discriminator learning rate
g_lr = 0.0003 # generator learning rate
batch_size = 8
train_epochs = 100
valid_epoch = 20
save_epoch = 100 # each save_epoch epochs model will be saved
verbose = False # if True some statistics will be printed during training

# declare device and generator, discriminator modules
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
red = Discriminator_red()
red.to(device)
inpainter = Inpaint()
inpainter.to(device)
red.train()
inpainter.train()
# optimizers
optimizer_D = torch.optim.Adam(red.parameters(), lr=d_lr, betas=(0.5, 0.9))
optimizer_G = torch.optim.Adam(inpainter.parameters(), lr=g_lr, betas=(0.5, 0.9))
# 
# mask = make_sq_mask(256,64)
# mask = transforms.ToTensor()(mask).unsqueeze(dim=0).repeat(batch_size,1,1,1)
# mask = mask.to(device)

# Train and valid datasets
train_dataset = datasets.ImageFolder(train_path,
                                    transform= transforms.Compose([
                                    transforms.ToTensor(),
                                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
                                    ]))

valid_dataset  = datasets.ImageFolder(valid_path,
                                    transform= transforms.Compose([
                                    transforms.ToTensor(),
                                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
                                    ]))
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,
                                            shuffle=True, num_workers=4)
                                    
valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,
                                            shuffle=False, num_workers=4)

# iterator on dataloader
train_loader_iter = iter(train_dataloader)
# number of iterations for specified number of epochs
train_iter = train_epochs * len(train_dataloader)
valid_size = len(valid_dataset)
valid_iter = valid_epoch * len(valid_dataloader)
save_iter = save_epoch * len(train_dataloader)
# lists to gather statistis
Loss_D_train, Loss_G_train = [], []
Loss_D_val, Loss_G_val = [], []
ssim_scores = []
psnr_local = []
psnr_global = []

# path where all necessary files will be kept
model_path = save_path + f'/dlr_{d_lr}_glr_{g_lr}/'
try:
    os.mkdir(model_path)
except Exception as e:
    pass
# tensorboard writer
writer = SummaryWriter(model_path)

# main train part
for it in range(train_iter+1):
    # Get a batch from dataloader
    try:
        Y = train_loader_iter.next()[0]
    except StopIteration:
        train_loader_iter = iter(train_dataloader)
        Y = train_loader_iter.next()[0]
    
    # make a random mask
    mask_size = np.random.randint(64,128)
    mask = make_sq_mask(256, mask_size)
    mask = transforms.ToTensor()(mask).unsqueeze(dim=0).repeat(len(Y),1,1,1)
    mask = mask.to(device)

    # Y is scaled to [-1,1], mask is from [0,1]
    # rescale Y to [0,1], mask it and rescale to [-1,1]
    alpha = it/train_iter
    Y = Y.to(device)
    Y = 0.5 * Y + 0.5
    x_mask = Y * (1-mask)
    x_mask = (x_mask - 0.5) * 2
    Y = (Y - 0.5) * 2
    x_mask = torch.cat([x_mask, 1 - mask], dim=1)
    # forward pass
    image_coarse, image_gen, image_result = inpainter(x_mask, Y, mask)
    # calculate loss on generated result
    D_fake_red = red(image_result)
    Loss_gan = -torch.mean(D_fake_red)
    Loss_s_re = torch.mean(torch.abs(image_gen - Y))
    Loss_hat = torch.mean(torch.abs(image_coarse - Y))
    Loss_G = 0.1*Loss_gan + 10*Loss_s_re + 5*(1-alpha) * Loss_hat
    # zero grad both models as during backward RED is also used 
    # to compute loss and the gradients from RED's previous update will hurt
    optimizer_D.zero_grad()
    optimizer_G.zero_grad()
    Loss_G.backward()
    # write gradient norms of inpainter to tensorboard
    for tag, parm in inpainter.named_parameters():
        writer.add_scalars(tag +'grad', {tag+'grad': parm.grad.data.norm(2).item()}, it)
    optimizer_G.step()
    # forward RED with a real image
    optimizer_D.zero_grad()
    D_real_red = red(Y)
    Loss_D_real = torch.mean(F.relu(1-D_real_red))
    Loss_D_real.backward()
    # write gradient norms of red to tensorboard    
    for tag, parm in red.named_parameters():
        writer.add_scalars(tag +'grad', {tag+'grad_real': parm.grad.data.norm(2).item()}, it)
    optimizer_D.step()
    # forward RED with a generated image
    optimizer_D.zero_grad()
    D_fake_red = red(image_result.data)
    Loss_D_fake = torch.mean(F.relu(1 + D_fake_red))
    Loss_D_fake.backward()
    # write gradient norms of red to tensorboard    
    for tag, parm in red.named_parameters():
        writer.add_scalars(tag +'grad', {tag+'grad_fake': parm.grad.data.norm(2).item()}, it)
    optimizer_D.step()

    if verbose:
        print(f"Loss_gan: {Loss_G.item()}, image_gen_Y: {Loss_s_re.item()}, image_coarse_Y: {Loss_hat.item()}\
                  Loss_D_real: {Loss_D_real.item()}, Loss_D_fake: {Loss_D_fake.item()}")

    # Write all losses to tensorboard
    writer.add_scalars('Loss', {'Loss_gan': Loss_G.item()}, it)
    writer.add_scalars('Loss', {'image_gen_Y': Loss_s_re.item()}, it)
    writer.add_scalars('Loss', {'image_coarse_Y': Loss_hat.item()}, it)
    writer.add_scalars('Loss', {'Loss_D_real': Loss_D_real.item()}, it)
    writer.add_scalars('Loss', {'Loss_D_fake': Loss_D_fake.item()}, it)

    # validation part
    if it%valid_iter==0:
        red.eval()
        inpainter.eval()
        with torch.no_grad():
            # ssim, psnr scores of validation
            ssim_score = 0
            psnr_g = 0
            psnr_l = 0
            for i, val in enumerate(valid_dataloader):
                # generate a random mask
                mask_size = np.random.randint(64,128)
                mask = make_sq_mask(256, mask_size)
                mask = transforms.ToTensor()(mask).unsqueeze(dim=0).repeat(len(Y),1,1,1)
                mask = mask.to(device)
                # mask the ground truth and generate the masked part
                Y = val[0].to(device)                
                Y = 0.5 * Y + 0.5
                x_mask = Y * (1 - mask)
                x_mask = (x_mask - 0.5) * 2
                Y = (Y - 0.5) * 2
                x_mask = torch.cat([x_mask, 1-mask], dim=1)
                image_coarse, image_gen, image_result = inpainter(x_mask, Y, mask)

                if i == 0:
                    # save one batch from validation
                    save_image(make_grid(0.5 * image_result + 0.5), f'{model_path}it_{it}.png')

                # compute ssim score for batch
                for b in range(batch_size):     
                    gt = Y[b][0].detach().cpu().numpy()
                    gt = np.clip(0.5 * gt + 0.5, 0, 1)
                    pred = image_result[b][0].detach().cpu().numpy()
                    pred = np.clip(0.5 * pred + 0.5, 0, 1)
                    ssim_score += ssim(gt, pred, data_range=1, nwin_size=11, gaussian_weights=1.5, K_1=0.01, K_2=0.03)
                    
                # compute local and global psnr scores
                Y = torch.clamp(0.5 * Y + 0.5, 0, 1)
                image = torch.clamp(0.5 * image_gen + 0.5, 0, 1)
                mse = torch.mean((Y - image) ** 2)
                psnr_g += 10 * (torch.log10(1/mse))
                mask_pred = image * (1-mask)
                mask_real = Y * (1-mask)
                psnr_l += 10 * (torch.log10(1/(F.mse_loss(mask_real, mask_pred, reduction='sum') / (3*mask.sum()))))    

            # gather scores and write to tensorboard             
            ssim_scores.append(ssim_score.item()/valid_size)
            psnr_global.append(psnr_g.item()/valid_size)
            psnr_local.append(psnr_l.item()/valid_size)
            writer.add_scalars('Ssim', {'Ssim_score':ssim_scores[-1]}, it)
            writer.add_scalars('PSNR_G', {'PSNR_G': psnr_global[-1]}, it)
            writer.add_scalars('PSNR_L', {'PSNR_L': psnr_local[-1]}, it)
            if verbose:         
                print(f'Iter-{it}, ssim: {ssim_scores[-1]}, psnr_g: {psnr_global[-1]}, psnr_l: {psnr_local[-1]}')
        # back to train mode
        red.train()
        inpainter.train()
# print final losses and scores
print(f"Loss_gan: {Loss_gan.item()} \nimage_gen_Y: {Loss_s_re.item()},\n\
image_coarse_Y: {Loss_hat.item()},\nLoss_D_real: {Loss_D_real.item()},\n\
Loss_D_fake: {Loss_D_fake.item()}")

# save models
torch.save(inpainter.state_dict(), model_path + 'inpaint.pt')
torch.save(red.state_dict(), model_path + "red.pt")
writer.close()